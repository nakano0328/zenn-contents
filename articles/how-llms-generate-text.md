---
title: "LLMはどう文章を生成する？仕組みと注意点を今更ながら解説！"
emoji: "🙌"
type: "tech" # tech: 技術記事 / idea: アイデア
topics: ["LLM", "生成AI", "機械学習","AI"]
published: true
publication_name: "hibari_inc"
---

こんにちは！株式会社 HIBARI の中野と申します。

日々LLMを扱っているものの、「そもそもLLMがどうやって文章を作っているのか」を体系的に説明できるようになりたい。
そんな思いから、改めて仕組みを整理してみました。これからLLMに触れる方はもちろん、すでに業務で使っている方にも振り返りの材料になれば幸いです。

# LLMの正体は予測マシン

スマホで「今日は天気が」と打つと、「良い」「悪い」「微妙」といった候補(予測変換)が出てきますよね。
LLMがやっていることを一言で表すなら、まさにこれです。
LLMとは、「**次に来る単語(トークン)をひたすら予測し続ける**」ための、超巨大で高性能な統計モデルなのです。

LLMは次の単語を「確率」で計算しています。
「この単語の次には、どの単語が来たら最も**それらしい**か？」を、過去に学習した膨大なデータから計算し、選び続けているに過ぎません。

これから、このそれらしい文章を作るプロセスを3つのステップで見ていきましょう。

## 1.文章を「部品」に分解する（トークン化）

まず、LLMは私たちが入力した文章を、そのまま「文章」として扱っていません。彼らは文章を、「**トークン(Token)**」という細かい部品に分解します。

例えば、「私はAIについて勉強中です。」という文章があったとします。
LLMはこれを、以下のように分解します。（※実際の分割方法はモデルによります）

`[ "私", "は", "AI", "について", "勉強", "中", "です", "。" ]`

LLMの世界では、すべての言葉がこの「トークン」として扱われます。「文法」や「意味」を直接理解しているわけではなく、「**このトークンの次には、あのトークンが来やすい**」というパターンのデータとして処理しているのです。

## 2.膨大な「教科書」でひたすら穴埋め問題を解く(事前学習)

LLMはどうやって「次にくるトークン」を予測しているのでしょうか？
それはLLMが学習した**膨大な**「**教科書**」(Webサイト、書籍、論文など、人類が書き溜めたテキストデータ)を読み込んでいるからです。

この「学習」とは、人間のように意味を理解することではありません。
ひたすら「穴埋め問題」を解き続ける作業に近いものです。

例えば、教科書に「吾輩は猫である。＿＿＿はまだ無い。」という文章があったとします。
LLMは「（名前）」というトークンを予測できるように訓練されます。

これを、文字通り「兆」を超えるレベルのパターンで実行します。
- 「日本の首都は」の次には「東京」が来やすい
- 「りんごの色は」の次には「赤色」が来やすい
  
この学習量が天文学的なレベルに達すると、モデルは単なる単語のつながりだけでなく、「文脈」や「事実関係」らしきものまで、統計的なパターンとして獲得していきます。

## 3.「次に来るトークン」を選び続ける(文章生成)

ここまでが学習です。では実際に私たちが質問を投げかけた時に、LLMの内部では何が起きているのでしょうか。

私たちが「AIが文章を作れる仕組みは、」と入力したとします。

1. LLMが`[ "AI", "が", "文章", "を", "作れる", "仕組み", "は", "、" ]`というトークンの並びを受け取ります。

2. LLMが「この並びに続く確率が最も高いトークンは何か？」を計算します。
    - 候補1：「次」（確率30%）
    - 候補2：「簡単」（確率15%）
    - 候補3：「大規模」（確率10%） ...など
  
  ここからLLMは、最も確率の高かった「次」を選びます。
  これで、LLMの中の文章は「...仕組みは、次」になりました。

3. LLMが「『...仕組みは、次』に続く確率が最も高いトークンは何か？」を再び計算します。
    - 候補1：「の」（確率40%）
    - 候補2：「に」（確率20%） ...など

  ここからLLMは、最も確率の高かった「の」を選びます。
  これで、LLMの中の文章は「...仕組みは、次の」になりました。

4. LLMが「『...仕組みは、次の』に続く...」を再び計算します。

...

このようにLLMは、この「計算 → 1トークン選択 → 文章に足す」という作業を、繰り返しているだけなのです。
そして、「。（句点）」や「[END]（終了トークン）」のように、「ここで文章を終えるのが最も確率が高い」と判断されるまで、これが自動的に続きます。
これが、私たちの目には「AIがスラスラと文章を書いている」ように見えているものの正体です。

# LLMの落とし穴: ハルシネーション

以上のようにLLMは「意味」を理解しているのではなく、あくまで**確率的にそれらしい単語の連なり**を生成しています。
この性質が、LLMの弱点である「**ハルシネーション**(幻覚)」を引き起こす原因になります。

ハルシネーションとは、LLMが**事実に基づかない情報をもっともらしく語ってしまう現象**です。
例えば「徳川家康の好きなラーメンは?」という質問に対して「醤油ラーメンです」と返すかもしれませんが、これは史実の裏付けがなく、完全な作り話です。トークンの確率だけを頼りに文章を組み立てる以上、こうした**自信満々の誤り**は避けられません。

特に以下のケースではハルシネーションの発生率が高まります。
- 学習データにほとんど存在しないニッチな情報を尋ねる場合
- 最新ニュースのように、学習時点で存在しなかった事実を求める場合
- 曖昧な指示や価値判断を含む質問で、確率的にそれらしい言い回しが優先される場合

実務でLLMを活用する際は、ソースを引用させる、外部データベースで検証する、人間が最終確認する、といった手当てを組み合わせることが重要です。

# まとめ

LLMは、膨大なテキストを材料に「次に来るトークン」を確率的に選び続けることで文章を生成しています。表面上の理解力に見える振る舞いは、巨大な統計モデルが蓄積した関連性を高速に引き出している結果にすぎません。その一方で、意味を理解していないがゆえにハルシネーションという弱点も抱えます。

LLMを活かすには、この仕組みと性質を踏まえ、得意な領域で使いながら、誤りを補正する仕組みを組み合わせることが欠かせません。これを機に、身近なLLMの挙動をもう一度観察し、より良い付き合い方を模索していきましょう。
