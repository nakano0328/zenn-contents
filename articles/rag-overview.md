---
title: "RAGについて（未定）"
emoji: "🧠"
type: "tech" # tech: 技術記事 / idea: アイデア
topics: ["AI", "LLM", "機械学習", "自然言語処理", "RAG"]
published: false
---

初めまして！株式会社HIBARIの中野と申します。これからの取り組みとしてテックブログを書き始めることにしました。よろしくお願いします！

# RAGとは？
RAG(Retrieval-Augmented Generation)は日本語では「**検索拡張生成**」などと言われています。大規模言語モデル(LLM)の持つ文章生成能力と、外部の知識を検索する能力を組み合わせた技術です。

## なぜRAGが必要なのか
今までのLLMでは特定のドメインに特化した情報を答えることができません。例えば社内会議の情報や会社独自の知識などを答えることができません。LLM自体が学んでいないことを解答しようとするともっともらしい嘘「ハルシネーション」をしてしまいます。RAGはこれらを解決することができます。

# RAGの仕組み
RAGの処理のフローは大きく分けて「**検索** (Retriever)」と「**生成** (Generator)」の２つに分かれます。

まずはユーザーからの質問をベクトルに変換します。このベクトルを検索します。次に検索した情報と元の質問を合体してLLMへ新しいプロンプトを入力します。

![](/images/rag-overview/rag.png)

## 検索(Retriever)
RAGにおける検索は、単なるキーワード検索ではありません。文章の「意味」を理解し、関連性の高い情報を見つけ出そうとしています。

そのために、**チャンクへの分割**、**ベクトル化** (Embedding)、**ベクトルデータベース**という3つの要素が連携しています。

### チャンクへの分割
LLMに参照させたいデータは数百ページに及ぶ書類や、長文のWebサイトなど、とにかく非常に大きいです。このドキュメントをそのまま扱おうとすると**検索精度が落ちたり**、LLMには入力できる量が決まっているので、そもそも**LLMが処理出来なかったり**と、問題が生じます。

そこで**チャンキング** (Chanking)という前処理を行います。

**チャンキング** (Chunking)とは、大きなドキュメントを意味のある小さな塊(**チャンク**)に分割することです。

例えば以下のような単位で分割します。
- 段落ごと
- 見出しごと
- 文字数や単語数

このチャンク単位で情報を扱うことで検索精度と効率が向上します。

### ベクトル化(Embedding)
ベクトル化(Embedding)とは、分割したチャンク1つひとつを、その意味をとらえた数値の羅列(**ベクトル**)に変換することです。この変換は、「埋め込みモデル(**Embedding Model**)」というAIモデルが行います。

以下の2つの特徴があります。

1. 意味の近さを距離で表現
   意味の近いチャンクはベクトル空間上で近くに配置されます。例えば「東京」と「大阪」は意味的に似ているので近くに配置されます。

2. 文脈を理解できる
   キーワードが一致していなくても、文脈が一致していれば関連性が高いと判断できる。

### データベース
RAGが参照する情報は「**ベクトルデータベース**」と呼ばれる専用のデータベースに保存されます。

このデータベースには、社内ドキュメントや議事録、問い合わせ履歴など、**LLMに参照させたいデータ**を格納します。この格納する際に「**ベクトル化** (Embedding)」を行います。これにより関連性の高い情報を見つけやすくなります。

## 生成(Generator)

# ファインチューニングとの違い

# RAGの活用例

# RAGのメリット・デメリット

# まとめ

# 参考文献
P. Lewis et al. "Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks." In Advances in Neural Information Processing Systems, vol. 33, pp. 9459-9474. Curran Associates, Inc., 2020.

新納 浩幸 (2024). 『LLMのファインチューニングとRAG: チャットボット開発による実践』 オーム社.